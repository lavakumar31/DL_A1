{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuHPK1ZYd2rd"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZTlukmYdAXp",
        "outputId": "5d404cd7-7a78-457e-f8aa-3ff6a9e877dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 286 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 358 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 368 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 399 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 430 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 440 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 460 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 471 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 501 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 512 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 532 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 552 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 573 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 583 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 604 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 614 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 624 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 655 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 665 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 675 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 686 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 716 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 727 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 737 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 747 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 768 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 778 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 788 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 798 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 808 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 819 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 829 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 839 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 849 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 870 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 880 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 890 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 901 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 921 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 931 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 942 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 952 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 962 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 972 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 983 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 993 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.7 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.7 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.6-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=3fe627d1c7ed597fea9420e32be7db3f8fe9398e745203456e12638f204afc9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.6 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "get_ipython().system('wandb login')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8-tmqWcfx7N",
        "outputId": "be9c249c-e1cb-40aa-969e-2547fd7a3f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init()\n",
        "# ... calculate metrics, generate media\n",
        "wandb.log({\"accuracy\": 0.9})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DebkmLnJklDM",
        "outputId": "94dd0b31-47eb-4026-8628-554e7f9b9126"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlava\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/lava/uncategorized/runs/p7jnex6h\" target=\"_blank\">blooming-glitter-1</a></strong> to <a href=\"https://wandb.ai/lava/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L4V1BSlo-Vx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE8WoMZneYJ5",
        "outputId": "2ca39072-b834-48b8-99e2-9a1844b6ace5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,)\n",
            "(784, 6000) (1, 60000)\n"
          ]
        }
      ],
      "source": [
        "((x_train,y_train),(x_test,y_test)) = fashion_mnist.load_data()\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "x = x_train.reshape(x_train.shape[1]*x_train.shape[2],x_train.shape[0])[:,0:6000]\n",
        "y_true = y_train.reshape(1,y_train.shape[0])[0:6000]\n",
        "\n",
        "y_k = y_train[0:6000]\n",
        "wt = [0,0]\n",
        "\n",
        "#x_train = np.array([[0,0,0],[0,0,1],[0,1,0],[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
        "#y_train = np.array([0,1,1,1,0,0,1])\n",
        "\n",
        "print(x.shape, y_true.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hot = np.zeros([len(set(y_k)),len(y_k)])\n",
        "for i in range(y_hot.shape[1]):\n",
        "  y_hot[y_k[i]][i] = 1\n",
        "print(y_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCGR1mhgXaO0",
        "outputId": "66b22fd7-53b5-4376-9a19-f60a7920e3d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 6000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nSTdDThfBOm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A class to create an object of one layer along with parameters to specifiy:\n",
        "  input dimensions,\n",
        "  number of nodes,\n",
        "  activation function,\n",
        "  randomMultiplier - Intial scaling factor for weights\n",
        "\"\"\"\n",
        "class Layer:\n",
        "    \n",
        "    def __init__(self, inputDimension, noOfNodes, activation='',optimizer='standard', randomMultiplier=0.01):\n",
        "        self.weights, self.bias = self.initialize(inputDimension, noOfNodes, randomMultiplier)\n",
        "        #self.prevMW, self.prevMb = np.zeros([noOfNodes,inputDimension]),np.ones([noOfNodes, 1])\n",
        "        #self.prevVW, self.prevVb = np.zeros([noOfNodes,inputDimension]),np.ones([noOfNodes, 1])\n",
        "\n",
        "        # Optimization Alogorithm for gradient Descent\n",
        "        if optimizer == 'standard':\n",
        "            self.optimizer = self.standard\n",
        "        elif optimizer == 'momentumGD':\n",
        "            self.optimizer = self.momentumGD\n",
        "        elif optimizer == 'rmsprop':\n",
        "            self.optimizer = self.rmsprop \n",
        "        elif optimizer == 'adam':\n",
        "            self.optimizer = self.adam\n",
        "        elif optimizer == 'nadam':\n",
        "            self.optimizer = self.nadam\n",
        "\n",
        "\n",
        "        # Activation Function for each layer\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = activation\n",
        "            self.activationForward = self.sigmoid\n",
        "            self.activationBackward = self.sigmoidGrad\n",
        "        else:\n",
        "            self.activation = 'softmax' \n",
        "            self.activationForward = self.softmax\n",
        "            self.activationBackward = self.softmaxGrad\n",
        "    \n",
        "    # Initialize the layer with some random weights and bias\n",
        "    def initialize(self, noOfInputFeatures, noOfNodes, randomMultiplier):\n",
        "        np.random.seed(0)\n",
        "        weights = randomMultiplier * np.random.randn(noOfNodes, noOfInputFeatures)\n",
        "        bias = np.ones([noOfNodes, 1])\n",
        "        return weights, bias\n",
        "\n",
        "    def param_intitialize(self,inputDimension,noOfNodes):\n",
        "        self.prevMW, self.prevMb = np.zeros([noOfNodes,inputDimension]),np.ones([noOfNodes, 1])\n",
        "        self.prevVW, self.prevVb = np.zeros([noOfNodes,inputDimension]),np.ones([noOfNodes, 1])\n",
        "\n",
        "    # Sigmoid activation function\n",
        "    def sigmoid(self, Z):\n",
        "        A = 1 / (1 + np.exp(-Z))\n",
        "        return A\n",
        "\n",
        "    # Differential of sigmoid function with chain rule applied\n",
        "    def sigmoidGrad(self, dA):\n",
        "        s = 1 / (1 + np.exp(-self.prevZ))\n",
        "        dZ = dA * s * (1 - s)\n",
        "        return dZ\n",
        "\n",
        "    #SoftMax Activation Function\n",
        "    def softmax(self,Z):\n",
        "      expZ = np.exp(Z - np.max(Z))\n",
        "      A = expZ / expZ.sum(axis=0, keepdims=True)\n",
        "      return A\n",
        "\n",
        "    def softmaxGrad(self,dA):\n",
        "      #a = np.exp(self.prevZ)\n",
        "      #A = a / np.sum(a, axis=0, keepdims=True)\n",
        "      #dZ = dA + A\n",
        "      #return dZ\n",
        "      k = self.prevZ\n",
        "      return dA\n",
        "\n",
        "\n",
        "    # Take's the input vector A and does the forward pass using weights and bias\n",
        "    def forward(self, A):\n",
        "        Z = np.dot(self.weights, A) + self.bias\n",
        "        self.prevZ = Z\n",
        "        self.prevA = A\n",
        "        A = self.activationForward(Z)\n",
        "        return A\n",
        "    \n",
        "    # Take's the input vector derivative of A (dA) and does the backward pass\n",
        "    def backward(self, dA):\n",
        "        dZ = self.activationBackward(dA)\n",
        "        m = self.prevA.shape[1]\n",
        "        self.dW = 1 / m * np.dot(dZ, self.prevA.T) \n",
        "        self.db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        prevdA = np.dot(self.weights.T, dZ)\n",
        "        return prevdA\n",
        "\n",
        "    def sgd(self, dA):\n",
        "        dZ = self.activationBackward(dA)\n",
        "        prevdA = np.dot(self.weights.T, dZ)\n",
        "        m = self.prevA.shape[1]\n",
        "        for i in range(m):\n",
        "          self.dW = 1 / m * np.dot(dZ[:,i:i+1], self.prevA[:,i:i+1].T)   #arr[:,2]\n",
        "          self.db = 1 / m * (dZ[:,i:i+1])\n",
        "          self.weights = self.weights - (0.02 * self.dW)\n",
        "          self.bias = self.bias - (0.02 * self.db)\n",
        "        #prevdA = np.dot(self.weights.T, dZ)\n",
        "        return prevdA\n",
        "    \n",
        "    # Update's the weights and bias with the passed learning_rate\n",
        "    def standard(self, learning_rate,t):\n",
        "        self.weights = self.weights - learning_rate * self.dW\n",
        "        self.bias = self.bias - learning_rate * self.db\n",
        "    \n",
        "    def momentumGD(self, learning_rate,t, moving_rate=0.9):\n",
        "        self.prevMW = moving_rate * self.prevMW + (1-moving_rate) * self.dW\n",
        "        self.prevMb = moving_rate * self.prevMb + (1-moving_rate) * self.db\n",
        "        self.weights = self.weights - learning_rate * self.prevMW\n",
        "        self.bias = self.bias - learning_rate * self.prevMb\n",
        "        \n",
        "\n",
        "    def rmsprop(self, learning_rate,t, moving_rate = 0.9):\n",
        "        self.prevVW = moving_rate * self.prevVW + (1-moving_rate) * np.square(self.dW)\n",
        "        self.prevVb = moving_rate * self.prevVb + (1-moving_rate) * np.square(self.db)\n",
        "        self.weights = self.weights - learning_rate * np.divide(self.dW,np.sqrt(self.prevVW+(1e-8)))\n",
        "        self.bias = self.bias - learning_rate * np.divide(self.db,np.sqrt(self.prevVb+(1e-8)))\n",
        "\n",
        "\n",
        "    def adam(self,learning_rate ,t, beta1 = 0.9, beta2 = 0.999):\n",
        "        self.prevMW = beta1 * self.prevMW + (1-beta1)*self.dW\n",
        "        self.prevMb = beta1 * self.prevMb + (1-beta1)*self.db\n",
        "\n",
        "        self.prevVW = beta2 * self.prevVW + (1-beta2)*np.square(self.dW)\n",
        "        self.prevVb = beta2 * self.prevVb + (1-beta2)*np.square(self.db)\n",
        "        \n",
        "        self.prevMW = self.prevMW/(1-beta1**(t+1))\n",
        "        self.prevMb = self.prevMb/(1-beta1**(t+1))\n",
        "\n",
        "        self.prevVW = self.prevVW/(1-beta2**(t+1))\n",
        "        self.prevVb = self.prevVb/(1-beta2**(t+1))\n",
        "\n",
        "        self.weights = self.weights - learning_rate * np.divide(self.prevMW,np.sqrt(self.prevVW+(1e-8)))\n",
        "        self.bias = self.bias - learning_rate * np.divide(self.prevMb,np.sqrt(self.prevVb+(1e-8)))\n",
        "        \"\"\"\n",
        "        MWhat = self.prevMW/(1-beta1**(t+1))\n",
        "        Mbhat = self.prevMb/(1-beta1**(t+1))\n",
        "\n",
        "        VWhat = self.prevVW/(1-beta2**(t+1))\n",
        "        Vbhat = self.prevVb/(1-beta2**(t+1))\n",
        "\n",
        "        self.weights = self.weights - learning_rate * np.divide(MWhat,np.sqrt(VWhat+(1e-8)))\n",
        "        self.bias = self.bias - learning_rate * np.divide(Mbhat,np.sqrt(Vbhat+(1e-8)))\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def nadam(self,learning_rate ,t, beta1 = 0.9, beta2 = 0.999):\n",
        "        self.prevMW = beta1 * self.prevMW + (1-beta1)*self.dW\n",
        "        self.prevMb = beta1 * self.prevMb + (1-beta1)*self.db\n",
        "\n",
        "        self.prevVW = beta2 * self.prevVW + (1-beta2)*np.square(self.dW)\n",
        "        self.prevVb = beta2 * self.prevVb + (1-beta2)*np.square(self.db)\n",
        "\n",
        "        MWhat = (beta1 * self.prevMW / (1-beta1)) + (self.dW)   #(b1 * m[t] / (1.0 - b1)) + ((1 - b1) * g(t) / (1.0 - b1))\n",
        "        Mbhat = (beta1 * self.prevMb / (1-beta1)) + (self.db)\n",
        "\n",
        "        VWhat = (beta2 * self.prevVW) / (1-beta2)\n",
        "        Vbhat = (beta2 * self.prevVb) / (1-beta2)\n",
        "\n",
        "        self.weights = self.weights - learning_rate * np.divide(MWhat,np.sqrt(VWhat+(1e-8)))\n",
        "        self.bias = self.bias - learning_rate * np.divide(Mbhat,np.sqrt(Vbhat+(1e-8)))\n",
        "\n",
        "        #self.prevMW = (beta1 * self.prevMW / (1-beta1)) + (self.dW)\n",
        "        #self.prevMb = (beta1 * self.prevMb / (1-beta1)) + (self.db)\n",
        "\n",
        "        #self.prevVW = (beta2 * self.prevVW) / (1-beta2)\n",
        "        #self.prevVb = (beta2 * self.prevVb) / (1-beta2)\n",
        "\n",
        "        #self.weights = self.weights - learning_rate * np.divide(self.prevMW,np.sqrt(self.prevVW+(1e-8)))\n",
        "        #self.bias = self.bias - learning_rate * np.divide(self.prevMb,np.sqrt(self.prevVb+(1e-8)))\n",
        "        \n",
        "          \n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WCA0ktbfHrZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Neural Network which consists of all layers and some helper functions\n",
        "\"\"\"\n",
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self, loss, optimizer = 'standard', randomMultiplier = 0.01):\n",
        "        self.layers=[]\n",
        "        self.optimizer = optimizer\n",
        "        self.randomMultiplier = randomMultiplier\n",
        "        if loss=='cross_entropy':\n",
        "            self.lossFunction = self.cross_entropy\n",
        "            self.lossBackward = self.cross_entropy_Grad\n",
        "        elif loss == 'mean_square':\n",
        "            self.lossFunction = self.meanSquareError\n",
        "            self.lossBackward = self.meanSquareErrorGrad\n",
        "        else:\n",
        "            print('Invalid loss function')\n",
        "        self.loss=loss\n",
        "\n",
        "    # add layer to the NN with input dimensions, nodes & activation function\n",
        "    def addLayer(self, inputDimension=None, noOfNodes=1, activation=''):\n",
        "        if (inputDimension is None):\n",
        "            if (len(self.layers) == 0):\n",
        "                print('Invalid number of layers')\n",
        "            inputDimension = self.layers[-1].outputDimension()\n",
        "        layer = Layer(inputDimension, noOfNodes, activation,optimizer = self.optimizer, randomMultiplier=self.randomMultiplier)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def meanSquareError(self, Y, A):\n",
        "        loss = np.square(Y - A)\n",
        "        m = Y.shape[1]\n",
        "        cost = 1 / m * np.sum(loss)\n",
        "        return np.squeeze(cost)\n",
        "    \n",
        "    # Compute's mean square grad error, Y is true value & A is predicted value\n",
        "    def meanSquareErrorGrad(self, Y, A):\n",
        "        dA = -2 * (Y - A)\n",
        "        return dA\n",
        "\n",
        "    # Compute's mean square error where Y is true value and A is predicted value\n",
        "    def cross_entropy(self, Y, A):\n",
        "        m = Y.shape[1]\n",
        "        cost = -(1/m)*np.sum(Y*np.log(A))\n",
        "        return np.squeeze(cost)\n",
        "    \n",
        "    # Compute's mean square grad error, Y is true value & A is predicted value\n",
        "    def cross_entropy_Grad(self, Y, A):\n",
        "        dA = A-Y\n",
        "        return dA\n",
        "\n",
        "    # Wrapper function to get the cost or loss value of the predicted values\n",
        "    def cost(self, Y, A):\n",
        "        return self.lossFunction(Y, A)\n",
        "\n",
        "    # Forward pass the input vector X through all layers\n",
        "    def forward(self, X):\n",
        "        x = np.copy(X)\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "            \n",
        "    # Backward pass the true values and predicted values in reverse direction\n",
        "    def backward(self, Y, A):\n",
        "        dA = self.lossBackward(Y, A)\n",
        "        if(self.optimizer != 'sgd'):\n",
        "            for layer in reversed(self.layers):\n",
        "                dA = layer.backward(dA)\n",
        "        else:\n",
        "          for layer in reversed(self.layers):\n",
        "                dA = layer.sgd(dA)\n",
        "    \n",
        "    # Update weights and compute's gradient descent of all layers\n",
        "    def update(self, learning_rate=0.02,t=0):\n",
        "        for layer in self.layers:\n",
        "            layer.optimizer(learning_rate,t=0)\n",
        "\n",
        "\n",
        "    def fit(self,x,y):\n",
        "        layers=[]\n",
        "        x = x.reshape(x.shape[0],x.shape[1]*x.shape[2]).T\n",
        "        layers.append(x.shape[0])\n",
        "\n",
        "        # Adding hidden layer with sigmoid as activation function\n",
        "        hidden_layers = int(input(\"Enter number of Hidden Layers : \"))\n",
        "        for i in range(hidden_layers):\n",
        "          tmp = int(input(\"Enter number of nodes you want in Hidden Layer \"+str(i+1)+\" : \"))\n",
        "          layers.append(tmp)\n",
        "          self.addLayer(inputDimension=layers[i], noOfNodes=tmp, activation='sigmoid')\n",
        "\n",
        "        y_hot = np.zeros([len(set(y)),len(y)])\n",
        "\n",
        "        for i in range(y_hot.shape[1]):\n",
        "          y_hot[y[i]][i] = 1\n",
        "          \n",
        "\n",
        "        # Output layer just to collect output (not a hidden layer)\n",
        "        self.addLayer(inputDimension=layers[hidden_layers], noOfNodes=y_hot.shape[0], activation='softmax')\n",
        "        for i in range(10):\n",
        "          A = self.forward(x)\n",
        "          self.backward(y_hot,A)\n",
        "          if(self.optimizer != 'sgd'):\n",
        "            self.update(learning_rate=0.01,t= i+1)\n",
        " \n",
        "          if not i%2 or ((y == y_pred).mean()*100>96):\n",
        "            print(\"After \",i+1,\"iterations:\")\n",
        "            print('cross Entropy:', model.cost(y_hot, A)) #2.3029423349010876\n",
        "            y_pred = np.argmax(A,axis = 0)\n",
        "            print('Training Accuracy:',(y == y_pred).mean()*100)\n",
        "\n",
        "            #for j in range(10):\n",
        "             # print(j,\"=>\",list(y_pred).count(j))\n",
        "\n",
        "\n",
        "            #break the loop when model is going to converge (accuracy: atleast 96%)\n",
        "\n",
        "            if ((y == y_pred).mean()*100>96):\n",
        "              #A = self.forward(x_test.reshape(x_test.shape[1]*x_test.shape[2],x_test.shape[0])[0:100])\n",
        "              #return A\n",
        "              break\n",
        "\n",
        "    def predict(self,x):\n",
        "        x = x.reshape(x.shape[0],x.shape[1]*x.shape[2]).T\n",
        "        A = self.forward(x)\n",
        "        A = np.argmax(A,axis=0)\n",
        "        return A\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gfHdgTlYcgmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "model = NeuralNetwork(loss='cross_entropy', optimizer = 'standard')\n",
        "y_pred = model.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arLAwRYtYzJY",
        "outputId": "1629a707-12e1-4760-96e8-fb4bb1bd8c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of Hidden Layers : 0\n",
            "After  1 iterations:\n",
            "cross Entropy: 47.37754644544284\n",
            "Training Accuracy: 7.673333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After  3 iterations:\n",
            "cross Entropy: nan\n",
            "Training Accuracy: 10.0\n",
            "After  5 iterations:\n",
            "cross Entropy: nan\n",
            "Training Accuracy: 10.0\n",
            "After  7 iterations:\n",
            "cross Entropy: nan\n",
            "Training Accuracy: 10.0\n",
            "After  9 iterations:\n",
            "cross Entropy: nan\n",
            "Training Accuracy: 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_p = model.predict(x_test)\n",
        "print((y_p == y_test).mean()*100)\n",
        "print(y_p)"
      ],
      "metadata": {
        "id": "mELK77lAhENt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a55b8a-ceb2-47b4-954e-3c20354f542c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_p = model.predict(x_train[0:100])\n",
        "y_pr = np.argmax(y_p,axis = 0)\n",
        "print((y_pr == y_train[0:100]).mean()*100)\n",
        "print(y_pr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1jOwaBf6tJt",
        "outputId": "b1fbf5f6-1099-4fe5-c45f-28f44d768da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0\n",
            "0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy_of_A1_Q3_(1) (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}